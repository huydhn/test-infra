{
  "query": "with failed_jobs as (\n    SELECT\n        FIRST_VALUE(job.conclusion) OVER(\n            PARTITION BY CONCAT(w.name, ' / ', job.name)\n            ORDER BY\n                push.head_commit.timestamp ROWS BETWEEN 1 PRECEDING\n                AND 1 FOLLOWING\n        ) = 'success'\n        and NTH_VALUE(job.conclusion, 2) OVER(\n            PARTITION BY CONCAT(w.name, ' / ', job.name)\n            ORDER BY\n                push.head_commit.timestamp ROWS BETWEEN 1 PRECEDING\n                AND 1 FOLLOWING\n        ) = 'failure'\n        and LAST_VALUE(job.conclusion) OVER(\n            PARTITION BY CONCAT(w.name, ' / ', job.name)\n            ORDER BY\n                push.head_commit.timestamp ROWS BETWEEN 1 PRECEDING\n                AND 1 FOLLOWING\n        ) = 'success' as flaky,\n        job.id,\n        job.head_sha,\n        job.name as jobname,\n        w.id as workflow_id,\n        w.head_branch,\n        w.name as workflow_name,\n        w.run_attempt as workflow_run_attempt,\n    from\n        commons.workflow_job job\n        join commons.workflow_run w on w.id = job.run_id\n        and w.head_repository.full_name = 'pytorch/pytorch'\n        join push on push.head_commit.id = w.head_commit.id\n    where\n        job._event_time >= CURRENT_DATE() - HOURS(:numHours)\n        and w.head_branch = 'main'\n        and w.name in ('trunk', 'pull')\n        and job.name not like '%mem_leak_check%'\n        and job.name not like '%rerun_disabled_tests%'\n    order by\n        job._event_time\n),\nflaky_jobs as (\n    select\n        distinct *\n    from\n        failed_jobs\n        left join commons.job_annotation annotation on annotation.jobID = failed_jobs.id\n    where\n        (\n            failed_jobs.flaky\n            and annotation.annotation is NULL\n        )\n        or annotation.annotation = 'TEST_FLAKE'\n),\nflaky_tests as (\n    select\n        test_run.name,\n        test_run.file,\n        test_run.classname,\n        test_run.invoking_file,\n        *,\n        if (\n            test_run.failure is null,\n            test_run.error.message,\n            test_run.failure.message\n        ) as failure_or_err_message,\n        test_run._event_time as event_time\n    from\n        test_run_s3 test_run\n        join flaky_jobs job on test_run.job_id = job.id\n        and test_run.workflow_run_attempt = job.workflow_run_attempt\n    where\n        (\n            test_run.error is not null\n            or test_run.failure is not null\n        )\n        and test_run.file is not null\n    order by\n        test_run.name\n)\nselect\n    name,\n    classname as suite,\n    file,\n    invoking_file,\n    ARRAY_AGG(jobname) as jobNames,\n    ARRAY_AGG(id) as jobIds,\n    ARRAY_AGG(workflow_id) as workflowIds,\n    ARRAY_AGG(workflow_name) as workflowNames,\n    ARRAY_AGG(workflow_run_attempt) as runAttempts,\n    ARRAY_AGG(event_time) as eventTimes,\n    ARRAY_AGG(head_branch) as branches\nfrom\n    flaky_tests\nwhere\n    not REGEXP_LIKE(failure_or_err_message, :ignoreMessages)\ngroup by\n    name,\n    file,\n    classname,\n    invoking_file\norder by\n    name\n",
  "default_parameters": [
    {
      "name": "ignoreMessages",
      "type": "string",
      "value": "No CUDA GPUs are available"
    },
    {
      "name": "numHours",
      "type": "int",
      "value": "96"
    },
    {
      "name": "threshold",
      "type": "int",
      "value": "1"
    }
  ]
}