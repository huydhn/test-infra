{
  "query": "WITH most_recent_strict_commits AS (\n    SELECT\n        push.head_commit.id as sha,\n    FROM\n        commons.push\n    WHERE\n        push.ref = 'refs/heads/viable/strict'\n        AND push.repository.full_name = 'pytorch/pytorch'\n    ORDER BY\n        push._event_time DESC\n    LIMIT\n        3\n), workflows AS (\n    SELECT\n        id\n    FROM\n        commons.workflow_run w\n        INNER JOIN most_recent_strict_commits c on w.head_sha = c.sha\n    WHERE\n        w.name != 'periodic'\n),\njob AS (\n    SELECT\n        j.id\n    FROM\n        commons.workflow_job j\n        INNER JOIN workflows w on w.id = j.run_id\n    WHERE\n        j.name NOT LIKE '%asan%'\n),\nduration_per_job AS (\n    SELECT\n        test_run.classname,\n        test_run.name,\n        job.id,\n        SUM(time) as time\n    FROM\n        commons.test_run_s3 test_run\n        /* `test_run` is ginormous and `job` is small, so lookup join is essential */\n        INNER JOIN job ON test_run.job_id = job.id HINT(join_strategy = lookup)\n    WHERE\n        /* cpp tests do not populate `file` for some reason. */\n        /* Exclude them as we don't include them in our slow test infra */\n        test_run.file IS NOT NULL\n        /* do some more filtering to cut down on the test_run size */\n        AND test_run.skipped IS NULL\n        AND test_run.failure IS NULL\n        AND test_run.error IS NULL\n    GROUP BY\n        test_run.classname,\n        test_run.name,\n        job.id\n)\nSELECT\n    CONCAT(\n        name,\n        ' (__main__.',\n        classname,\n        ')'\n    ) as test_name,\n    AVG(time) as avg_duration_sec\nFROM\n    duration_per_job\nGROUP BY\n    CONCAT(\n        name,\n        ' (__main__.',\n        classname,\n        ')'\n    )\nHAVING\n    AVG(time) > 60.0\nORDER BY\n    test_name\n",
  "default_parameters": []
}