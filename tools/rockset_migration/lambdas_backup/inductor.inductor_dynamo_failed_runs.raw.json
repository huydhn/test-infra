{
  "workspace": "inductor",
  "last_updated_by": "binbao@meta.com",
  "last_updated": "2023-05-04T20:51:03Z",
  "name": "inductor_dynamo_failed_runs",
  "version_count": 2,
  "collections": [
    "commons.workflow_job",
    "commons.workflow_run"
  ],
  "latest_version": {
    "workspace": "inductor",
    "created_by": "binbao@meta.com",
    "created_by_apikey_name": null,
    "created_at": "2023-05-04T20:51:03Z",
    "name": "inductor_dynamo_failed_runs",
    "version": "0c29c448afd2963b",
    "description": "Failed inductor and dynamo runs",
    "sql": {
      "query": "with jobs_with_annotations as (\n  select\n      (    \n        (job.torchci_classification.line is not null and job.torchci_classification.line like '%No CUDA GPUs are available%')\n        or \n        ( -- Ignore nvidia driver installation failures\n          LENGTH(job.steps) > 5\n          and job.steps[6].name like 'Install nvidia driver%'\n          and job.steps[6].conclusion = 'failure' \n        )\n      ) as gpu_failure,\n      job._event_time,\n      job.head_sha as sha,\n      FIRST_VALUE(job.conclusion) OVER(\n          PARTITION BY CONCAT(w.name, ' / ', job.name)\n          ORDER BY\n              job._event_time ROWS BETWEEN 1 PRECEDING\n              AND 1 FOLLOWING\n      ) = 'success'\n      and NTH_VALUE(job.conclusion, 2) OVER(\n          PARTITION BY CONCAT(w.name, ' / ', job.name)\n          ORDER BY\n              job._event_time ROWS BETWEEN 1 PRECEDING\n              AND 1 FOLLOWING\n      ) = 'failure'\n      and LAST_VALUE(job.conclusion) OVER(\n          PARTITION BY CONCAT(w.name, ' / ', job.name)\n          ORDER BY\n              job._event_time ROWS BETWEEN 1 PRECEDING\n              AND 1 FOLLOWING\n      ) = 'success' as flaky,\n      job.id,\n      w.head_branch,\n      CONCAT(w.name, ' / ', job.name) as jobName,\n      w.name as workflowName,\n      job.conclusion,\n      job.html_url as htmlUrl,\n      CONCAT(\n          'https://ossci-raw-job-status.s3.amazonaws.com/log/',\n          CAST(job.id as string)\n      ) as logUrl,\n      DATE_DIFF(\n          'SECOND',\n          PARSE_TIMESTAMP_ISO8601(job.started_at),\n          PARSE_TIMESTAMP_ISO8601(job.completed_at)\n      ) as durationS,\n      w.repository.full_name as repo,\n      job.torchci_classification.line as failureLine,\n      job.torchci_classification.captures as failureCaptures,\n      job.torchci_classification.line_num as failureLineNumber,\n      job.steps,\n      job.conclusion = 'success' as successful\n  from\n      commons.workflow_job job\n      join commons.workflow_run w on w.id = job.run_id\n      and w.head_repository.full_name = 'pytorch/pytorch'\n  where\n      -- w.head_branch = :branch\n      -- and\n      job._event_time >= CURRENT_DATE() - INTERVAL 2 WEEK\n      and w.head_repository.full_name = 'pytorch/pytorch'\n      and w.head_branch = 'main'\n      and (\n          job.name like '%dynamo%'\n          or w.name like '%dynamo%'\n          or job.name like '%inductor%'\n          or w.name like '%inductor%'\n      )\n      and job.conclusion in ('success', 'failure')\n  order by\n      job._event_time\n-- The specific runs that flaked\n)\n  \n-- The specific runs that flaked\nselect _event_time, sha, failureLine, steps,  jobName, workflowName, htmlUrl, logUrl, durationS\nfrom jobs_with_annotations \nwhere \n  not successful and not gpu_failure\norder by _event_time desc\n\n\n",
      "default_parameters": []
    },
    "collections": [
      "commons.workflow_run",
      "commons.workflow_job"
    ],
    "state": "ACTIVE",
    "stats": {
      "last_executed": null,
      "last_executed_by": null,
      "last_execution_error": null,
      "last_execution_error_message": null
    },
    "public_access_id": null
  }
}