{
  "query": "WITH all_jobs AS (\n    SELECT\n        push._event_time as time,\n        job.conclusion AS conclusion,\n        push.head_commit.id AS sha,\n        ROW_NUMBER() OVER(PARTITION BY job.name, push.head_commit.id ORDER BY job.run_attempt DESC) AS row_num,\n    FROM\n        push\n        JOIN commons.workflow_run workflow ON workflow.head_commit.id = push.head_commit.id\n        JOIN commons.workflow_job job ON workflow.id = job.run_id\n    WHERE\n        job.name != 'ciflow_should_run'\n        AND job.name != 'generate-test-matrix'\n        AND ( -- Limit it to workflows which block viable/strict upgrades\n            ARRAY_CONTAINS(SPLIT(:workflowNames, ','), LOWER(workflow.name))\n            OR workflow.name like 'linux-binary%'\n        )\n        AND job.name NOT LIKE '%rerun_disabled_tests%'\n        AND job.name NOT LIKE '%unstable%'\n        AND workflow.event != 'workflow_run' -- Filter out worflow_run-triggered jobs, which have nothing to do with the SHA\n        AND push.ref IN ('refs/heads/master', 'refs/heads/main')\n        AND push.repository.owner.name = 'pytorch'\n        AND push.repository.name = 'pytorch'\n        AND push._event_time >= PARSE_DATETIME_ISO8601(:startTime)\n        AND push._event_time < PARSE_DATETIME_ISO8601(:stopTime)\n    UNION ALL\n    SELECT\n        push._event_time as time,\n        CASE\n            WHEN job.job.status = 'failed' then 'failure'\n            WHEN job.job.status = 'canceled' then 'cancelled'\n            ELSE job.job.status\n        END AS conclusion,\n        push.head_commit.id AS sha,\n        ROW_NUMBER() OVER(PARTITION BY job.name, push.head_commit.id ORDER BY job.run_attempt DESC) AS row_num,\n    FROM\n        circleci.job job\n        JOIN push ON job.pipeline.vcs.revision = push.head_commit.id\n    WHERE\n        push.ref IN ('refs/heads/master', 'refs/heads/main')\n        AND push.repository.owner.name = 'pytorch'\n        AND push.repository.name = 'pytorch'\n        AND push._event_time >= PARSE_DATETIME_ISO8601(:startTime)\n        AND push._event_time < PARSE_DATETIME_ISO8601(:stopTime)\n),\nany_red AS (\n    SELECT\n        FORMAT_TIMESTAMP('%Y-%m-%d', DATE_TRUNC(:granularity, time)) AS granularity_bucket,\n        sha,\n        CAST(\n            SUM(\n                CASE\n                    WHEN conclusion = 'failure' THEN 1\n                    WHEN conclusion = 'timed_out' THEN 1\n                    WHEN conclusion = 'cancelled' THEN 1\n                    ELSE 0\n                END\n            ) > 0 AS int\n        ) AS all_red,\n        CAST(\n            SUM(\n                CASE\n                    WHEN conclusion = 'failure' AND row_num = 1 THEN 1\n                    WHEN conclusion = 'timed_out' AND row_num = 1 THEN 1\n                    WHEN conclusion = 'cancelled' AND row_num = 1 THEN 1\n                    ELSE 0\n                END\n            ) > 0 AS int\n        ) AS broken_trunk_red,\n    FROM\n        all_jobs\n    GROUP BY\n        granularity_bucket,\n        sha\n    HAVING\n        count(sha) > 10 -- Filter out jobs that didn't run anything.\n        AND SUM(IF(conclusion is NULL, 1, 0)) = 0 -- Filter out commits that still have pending jobs.\n),\nclassified_red AS (\n    SELECT\n        granularity_bucket,\n        ARRAY_CREATE(\n            ARRAY_CREATE('Broken trunk', AVG(broken_trunk_red)),\n            ARRAY_CREATE('Flaky', AVG(all_red) - AVG(broken_trunk_red)),\n            ARRAY_CREATE('Total', AVG(all_red))\n        ) AS metrics,\n    FROM\n        any_red\n    GROUP BY\n        granularity_bucket\n),\navg_red AS (\n    SELECT\n        classified_red.granularity_bucket,\n        ELEMENT_AT(metrics.metric, 1) AS name,\n        ELEMENT_AT(metrics.metric, 2) AS metric,\n    FROM\n        classified_red\n        CROSS JOIN UNNEST(classified_red.metrics AS metric) AS metrics\n    ORDER BY\n        granularity_bucket DESC\n)\nSELECT\n    granularity_bucket,\n    name,\n    -- 2 week rolling average\n    (\n        SUM(metric) OVER(\n            PARTITION BY name\n            ORDER BY\n                granularity_bucket ROWS 1 PRECEDING\n        )\n    ) / 2.0 AS metric,    \nFROM\n    avg_red",
  "default_parameters": [
    {
      "name": "granularity",
      "type": "string",
      "value": "week"
    },
    {
      "name": "startTime",
      "type": "string",
      "value": "2023-02-01T00:00:00.000Z"
    },
    {
      "name": "stopTime",
      "type": "string",
      "value": "2023-03-30T00:00:00.000Z"
    },
    {
      "name": "workflowNames",
      "type": "string",
      "value": "lint,pull,trunk"
    }
  ]
}