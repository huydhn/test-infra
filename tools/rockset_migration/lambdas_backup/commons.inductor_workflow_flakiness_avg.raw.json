{
  "workspace": "commons",
  "last_updated_by": "binbao@meta.com",
  "last_updated": "2023-05-04T20:50:05Z",
  "name": "inductor_workflow_flakiness_avg",
  "version_count": 2,
  "collections": [
    "commons.workflow_job",
    "commons.workflow_run"
  ],
  "latest_version": {
    "workspace": "commons",
    "created_by": "binbao@meta.com",
    "created_by_apikey_name": null,
    "created_at": "2023-05-04T20:50:05Z",
    "name": "inductor_workflow_flakiness_avg",
    "version": "9f07e118fb7d9ea0",
    "description": "Average flakiness induced by workflow jobs on trunk in the past two weeks",
    "sql": {
      "query": "with jobs_with_flaky_annotation as (\n    select\n        job._event_time,\n        job.head_sha as sha,\n        FIRST_VALUE(job.conclusion) OVER(\n            PARTITION BY CONCAT(w.name, ' / ', job.name)\n            ORDER BY\n                job._event_time ROWS BETWEEN 1 PRECEDING\n                AND 1 FOLLOWING\n        ) = 'success'\n        and NTH_VALUE(job.conclusion, 2) OVER(\n            PARTITION BY CONCAT(w.name, ' / ', job.name)\n            ORDER BY\n                job._event_time ROWS BETWEEN 1 PRECEDING\n                AND 1 FOLLOWING\n        ) = 'failure'\n        and LAST_VALUE(job.conclusion) OVER(\n            PARTITION BY CONCAT(w.name, ' / ', job.name)\n            ORDER BY\n                job._event_time ROWS BETWEEN 1 PRECEDING\n                AND 1 FOLLOWING\n        ) = 'success' as flaky,\n        job.id,\n        w.head_branch,\n        CONCAT(w.name, ' / ', job.name) as jobName,\n        w.name as workflowName,\n        job.conclusion,\n        job.html_url as htmlUrl,\n        CONCAT(\n            'https://ossci-raw-job-status.s3.amazonaws.com/log/',\n            CAST(job.id as string)\n        ) as logUrl,\n        DATE_DIFF(\n            'SECOND',\n            PARSE_TIMESTAMP_ISO8601(job.started_at),\n            PARSE_TIMESTAMP_ISO8601(job.completed_at)\n        ) as durationS,\n        w.repository.full_name as repo,\n        job.torchci_classification.line as failureLine,\n        job.torchci_classification.captures as failureCaptures,\n        job.torchci_classification.line_num as failureLineNumber,\n    from\n        commons.workflow_job job\n        join commons.workflow_run w on w.id = job.run_id\n        and w.head_repository.full_name = 'pytorch/pytorch'\n    where\n        -- w.head_branch = :branch\n        -- and\n        job._event_time >= CURRENT_DATE() - INTERVAL 2 WEEK\n        and w.head_repository.full_name = 'pytorch/pytorch'\n        and w.head_branch = 'main'\n        and (\n            job.name like '%inductor%'\n            or w.name like '%inductor%'\n        )\n        and job.conclusion in ('success', 'failure')\n    order by\n        job._event_time\n),\nfailed_due_to_flaky_inductor as (\n    select\n        sha,\n        max(flaky) as flaky\n    from\n        jobs_with_flaky_annotation\n    group by\n        sha\n)\n\n-- Overall flakiness on trunk\nselect (select count(*) from failed_due_to_flaky_inductor where flaky) * 100.0/(select count(*) from failed_due_to_flaky_inductor)\n\n-- The flakiest jobs\n-- select AVG(CAST(flaky as FLOAT)) * 100.0 as failure_rate, jobName from jobs_with_flaky_annotation group by jobName order by failure_rate desc\n\n-- The specific runs that flaked\n-- select * from jobs_with_flaky_annotation where flaky\n",
      "default_parameters": []
    },
    "collections": [
      "commons.workflow_run",
      "commons.workflow_job"
    ],
    "state": "ACTIVE",
    "stats": {
      "last_executed": null,
      "last_executed_by": null,
      "last_execution_error": null,
      "last_execution_error_message": null
    },
    "public_access_id": null
  }
}