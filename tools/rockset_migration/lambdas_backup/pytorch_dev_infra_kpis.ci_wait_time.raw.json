{
  "workspace": "pytorch_dev_infra_kpis",
  "last_updated_by": "zainriz3@gmail.com",
  "last_updated": "2023-04-10T21:49:05Z",
  "name": "ci_wait_time",
  "version_count": 1,
  "collections": [
    "metrics.pr_stats"
  ],
  "latest_version": {
    "workspace": "pytorch_dev_infra_kpis",
    "created_by": "zainriz3@gmail.com",
    "created_by_apikey_name": null,
    "created_at": "2023-04-10T21:49:05Z",
    "name": "ci_wait_time",
    "version": "b1080f26b20ea142",
    "description": "Common percentiles for how much time CI is running per PR and how many commits a PR has",
    "sql": {
      "query": "WITH percentiles_desired AS (\n  -- All the percentiles that we want the query to return\n  SELECT 'p25' as percentile, 0.25 as percentile_num\n  UNION ALL\n  SELECT 'p50', 0.50\n  UNION ALL\n  SELECT 'p75', 0.75\n  UNION ALL\n  SELECT 'p90', 0.90\n),\n-- Set the bucket to the desired granularity\ngranular_pr_stats as (\n    SELECT\n      DATE_TRUNC(:granularity, end_time) AS bucket,\n      *\n    FROM metrics.pr_stats\n),\n-- Within each bucket, figure out what percentile duration and num_commits each PR falls under\npercentiles as (\n  SELECT\n      pr_number,\n      bucket,\n      duration_mins,\n      PERCENT_RANK() OVER(\n          PARTITION BY bucket\n          ORDER by duration_mins\n      ) as duration_percentile,\n      num_commits,\n      PERCENT_RANK() OVER(\n          PARTITION BY bucket\n          ORDER by num_commits\n      ) as num_commits_percentile\n  FROM\n      granular_pr_stats\n  WHERE 1 = 1\n    AND PARSE_DATETIME_ISO8601(:startTime) <= bucket\n    AND DATE(PARSE_DATETIME_ISO8601(:stopTime)) >= bucket \n),\n-- For each bucket, get just the durations corresponding to the desired percentiles\nduration_percentile as (\n  SELECT \n    p.bucket,\n    pd.percentile,\n    MIN(p.duration_mins) as duration_mins\n  FROM percentiles p \n    CROSS JOIN percentiles_desired pd \n  WHERE p.duration_percentile >= pd.percentile_num\n  GROUP BY\n   p.bucket, pd.percentile\n),\n-- For each bucket, get just the number of commits corresponding to the desired percentiles\nnum_commits_percentile as (\n  SELECT \n    p.bucket,\n    pd.percentile,\n    MIN(p.num_commits) as num_commits\n  FROM percentiles p \n    CROSS JOIN percentiles_desired pd \n  WHERE p.num_commits_percentile >= pd.percentile_num\n  GROUP BY\n   p.bucket, pd.percentile\n)\n-- Keep the percentiles on the same row so that this one query can give the results for both types of data\nSELECT \n  FORMAT_TIMESTAMP('%Y-%m-%d', d.bucket) as bucket,\n  d.percentile,\n  d.duration_mins,\n  c.num_commits\nFROM \n  duration_percentile d \n  INNER JOIN num_commits_percentile c on d.bucket = c.bucket and d.percentile = c.percentile\nWHERE\n  d.bucket < (SELECT max(bucket) from granular_pr_stats) -- discard the latest bucket, which will have noisy, partial data\nORDER BY bucket DESC, duration_mins\n",
      "default_parameters": [
        {
          "name": "granularity",
          "type": "string",
          "value": "week"
        },
        {
          "name": "startTime",
          "type": "string",
          "value": "2022-05-01T00:00:00.000Z"
        },
        {
          "name": "stopTime",
          "type": "string",
          "value": "2023-06-01T00:06:32.839Z"
        }
      ]
    },
    "collections": [
      "metrics.pr_stats"
    ],
    "state": "ACTIVE",
    "stats": {
      "last_executed": "2024-06-25T13:36:24Z",
      "last_executed_by": "darthsuo@gmail.com",
      "last_execution_error": "2024-03-05T08:49:15Z",
      "last_execution_error_message": "RESOURCE_EXHAUSTED: The instance size you are using does not have the memory capacity to execute the current workload. Please upgrade to a larger instance size, or reduce the rate or complexity of the queries you are running."
    },
    "public_access_id": null
  }
}