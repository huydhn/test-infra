{
  "workspace": "metrics",
  "last_updated_by": "huydhn@gmail.com",
  "last_updated": "2022-11-04T20:56:23Z",
  "name": "tts_duration_historical_percentile",
  "version_count": 1,
  "collections": [
    "commons.workflow_job",
    "commons.workflow_run"
  ],
  "latest_version": {
    "workspace": "metrics",
    "created_by": "huydhn@gmail.com",
    "created_by_apikey_name": null,
    "created_at": "2022-11-04T20:56:23Z",
    "name": "tts_duration_historical_percentile",
    "version": "f6824cbe03e1b6d8",
    "description": "Query both TTS and duration percentiles and group them at different granularity",
    "sql": {
      "query": "SELECT\n    granularity_bucket,\n    MAX(tts_sec) AS tts_percentile_sec,\n    MAX(duration_sec) AS duration_percentile_sec,\n    full_name\nFROM (\n    SELECT\n        granularity_bucket,\n        tts_sec,\n        PERCENT_RANK() OVER (PARTITION BY full_name ORDER BY tts_sec DESC) AS tts_percentile,\n        duration_sec,\n        PERCENT_RANK() OVER (PARTITION BY full_name ORDER BY duration_sec DESC) AS duration_percentile,\n        full_name,\n    FROM (\n        SELECT\n            FORMAT_ISO8601(\n                DATE_TRUNC(\n                    :granularity,\n                    job._event_time AT TIME ZONE :timezone\n                )\n            ) AS granularity_bucket,\n            DATE_DIFF(\n                'second',\n                PARSE_TIMESTAMP_ISO8601(workflow.created_at) AT TIME ZONE :timezone,\n                PARSE_TIMESTAMP_ISO8601(job.completed_at) AT TIME ZONE :timezone\n            ) AS tts_sec,\n            DATE_DIFF(\n                'second',\n                PARSE_TIMESTAMP_ISO8601(job.started_at) AT TIME ZONE :timezone,\n                PARSE_TIMESTAMP_ISO8601(job.completed_at) AT TIME ZONE :timezone\n            ) AS duration_sec,\n            CONCAT(workflow.name, ' / ', job.name) as full_name\n        FROM\n            commons.workflow_job job\n            JOIN commons.workflow_run workflow on workflow.id = job.run_id\n        WHERE\n            job._event_time >= PARSE_DATETIME_ISO8601(:startTime)\n            AND job._event_time < PARSE_DATETIME_ISO8601(:stopTime)\n            AND ARRAY_CONTAINS(SPLIT(:workflowNames, ','), workflow.name)\n            AND workflow.head_branch LIKE :branch\n            AND workflow.run_attempt = 1\n    ) AS tts_duration\n) AS p\nWHERE\n    (SELECT p.tts_percentile >= (1.0 - :percentile) OR p.duration_percentile >= (1.0 - :percentile))\nGROUP BY\n    granularity_bucket,\n    full_name\nORDER BY\n    full_name ASC\n",
      "default_parameters": [
        {
          "name": "branch",
          "type": "string",
          "value": "%"
        },
        {
          "name": "granularity",
          "type": "string",
          "value": "day"
        },
        {
          "name": "percentile",
          "type": "float",
          "value": "0.9"
        },
        {
          "name": "startTime",
          "type": "string",
          "value": "2022-07-01T00:00:00.000Z"
        },
        {
          "name": "stopTime",
          "type": "string",
          "value": "2022-08-01T00:00:00.000Z"
        },
        {
          "name": "timezone",
          "type": "string",
          "value": "America/Los_Angeles"
        },
        {
          "name": "workflowNames",
          "type": "string",
          "value": "pull,trunk,nightly,periodic,inductor"
        }
      ]
    },
    "collections": [
      "commons.workflow_run",
      "commons.workflow_job"
    ],
    "state": "ACTIVE",
    "stats": {
      "last_executed": "2024-06-25T03:28:32Z",
      "last_executed_by": "darthsuo@gmail.com",
      "last_execution_error": "2024-03-05T06:54:27Z",
      "last_execution_error_message": "RESOURCE_EXHAUSTED: The instance size you are using does not have the memory capacity to execute the current workload. Please upgrade to a larger instance size, or reduce the rate or complexity of the queries you are running."
    },
    "public_access_id": null
  }
}